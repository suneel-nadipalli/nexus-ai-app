{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":8197806,"datasetId":4848781,"databundleVersionId":8321625}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -Uqqq pip --progress-bar off\nprint(\"Installed PIP\")\n!pip install -Uqqq git+https://github.com/huggingface/transformers\nprint(\"Installed Transformers\")\n!pip install -qqq -U peft\nprint(\"Installed PEFT\")\n!pip install -Uqqq accelerate\nprint(\"Installed Accelerate\")\n!pip install -qqq loralib==0.1.1 --progress-bar off\n!pip install -qqq einops==0.6.1 --progress-bar off\nprint(\"Installed LoRA Lib\")\n!pip install -qqq bitsandbytes\nprint(\"Installed BitsAndBytes\")\n!pip install fsspec -qqq\n\n!pip install datasets==2.16.0 fsspec==2023.10.0 gcsfs==2023.10.0 -qqq\nprint(\"Installed Datasets\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T22:59:43.057426Z","iopub.execute_input":"2024-04-22T22:59:43.057735Z","iopub.status.idle":"2024-04-22T23:03:13.062331Z","shell.execute_reply.started":"2024-04-22T22:59:43.057710Z","shell.execute_reply":"2024-04-22T23:03:13.061217Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Installed PIP\nInstalled Transformers\nInstalled PEFT\nInstalled Accelerate\nInstalled LoRA Lib\nInstalled BitsAndBytes\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mInstalled Datasets\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -Uqqq torch torchvision\nprint(\"Installed Torch\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:03:13.064395Z","iopub.execute_input":"2024-04-22T23:03:13.064744Z","iopub.status.idle":"2024-04-22T23:14:29.511315Z","shell.execute_reply.started":"2024-04-22T23:03:13.064709Z","shell.execute_reply":"2024-04-22T23:14:29.509922Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Installed Torch\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install huggingface_hub -qqq","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:29.513409Z","iopub.execute_input":"2024-04-22T23:14:29.518638Z","iopub.status.idle":"2024-04-22T23:14:42.337964Z","shell.execute_reply.started":"2024-04-22T23:14:29.518580Z","shell.execute_reply":"2024-04-22T23:14:42.336516Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nimport wandb\n\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"text-summarizer\")\nsecret_value_1 = user_secrets.get_secret(\"wandb\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:42.340496Z","iopub.execute_input":"2024-04-22T23:14:42.340808Z","iopub.status.idle":"2024-04-22T23:14:44.289746Z","shell.execute_reply.started":"2024-04-22T23:14:42.340777Z","shell.execute_reply":"2024-04-22T23:14:44.288643Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import huggingface_hub\n\nwandb.login(key = secret_value_1)\n\nhuggingface_hub.login(token=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:44.291086Z","iopub.execute_input":"2024-04-22T23:14:44.291504Z","iopub.status.idle":"2024-04-22T23:14:47.202191Z","shell.execute_reply.started":"2024-04-22T23:14:44.291465Z","shell.execute_reply":"2024-04-22T23:14:47.201250Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport os\nfrom pprint import pprint\nimport bitsandbytes as bnb\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import load_dataset\nfrom huggingface_hub import notebook_login\n\nfrom peft import (\n    LoraConfig,\n    PeftConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\n\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig  \n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:47.203626Z","iopub.execute_input":"2024-04-22T23:14:47.204132Z","iopub.status.idle":"2024-04-22T23:14:54.752863Z","shell.execute_reply.started":"2024-04-22T23:14:47.204097Z","shell.execute_reply":"2024-04-22T23:14:54.752029Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# MODEL_NAME = \"akoksal/LongForm-OPT-125M\"\n\nMODEL_NAME = \"microsoft/phi-2\"\n\nbnb_config = BitsAndBytesConfig(\n   load_in_4bit=True,\n   bnb_4bit_quant_type=\"nf4\",\n   bnb_4bit_use_double_quant=True,\n   bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map='auto',\n    quantization_config=bnb_config,\n    use_cache=False,\n    trust_remote_code=True\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:14:54.754145Z","iopub.execute_input":"2024-04-22T23:14:54.755021Z","iopub.status.idle":"2024-04-22T23:15:34.279039Z","shell.execute_reply.started":"2024-04-22T23:14:54.754981Z","shell.execute_reply":"2024-04-22T23:15:34.278129Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/863 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"352a767cc1274fb79366e09ddd399e7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi.py:   0%|          | 0.00/9.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a0c95d84db940238c91865335a024ff"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- configuration_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi.py:   0%|          | 0.00/62.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"220d170703244aaf9730319b49d6cc96"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/phi-2:\n- modeling_phi.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c1824eec6844c99563805987176988"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b797c21db290491f990411f8425f270f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c272f637646b4ca69984a8573f00db40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d928d4c3571c4e488f83a798c7586d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03f8099faf134332a71178e3930cce6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae508dbdad4542589bd818c08e4ae022"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e58de57929ce4db2829aea10797f9a38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78189157c6a54eb29e76be399e018796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccddbfc27f424c72a558c4d4b2a7aa5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd9b848baa147e0b5acbc90d14ea6a9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b94b56e0dc4cccbe26c0ba3bdeef9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820746eec1254564bc5bad1d7783f73c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"def print_trainable_parameters(model):\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        \n        if param.requires_grad:\n            trainable_params += param.numel()\n    \n    print(\n        f\"Trainable params: {trainable_params} || All params: {all_param} || trainable %: {100* trainable_params/all_param}\"\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:34.280538Z","iopub.execute_input":"2024-04-22T23:15:34.281005Z","iopub.status.idle":"2024-04-22T23:15:34.286999Z","shell.execute_reply.started":"2024-04-22T23:15:34.280964Z","shell.execute_reply":"2024-04-22T23:15:34.286045Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:34.288361Z","iopub.execute_input":"2024-04-22T23:15:34.288983Z","iopub.status.idle":"2024-04-22T23:15:38.003856Z","shell.execute_reply.started":"2024-04-22T23:15:34.288948Z","shell.execute_reply":"2024-04-22T23:15:38.002738Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n#     target_modules=['query_key_value'],\n    lora_dropout=0.05,\n    bias='none',\n    task_type='CAUSAL_LM'\n)\n\nmodel = get_peft_model(model, config)\nprint_trainable_parameters(model)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:38.008105Z","iopub.execute_input":"2024-04-22T23:15:38.008831Z","iopub.status.idle":"2024-04-22T23:15:38.500289Z","shell.execute_reply.started":"2024-04-22T23:15:38.008800Z","shell.execute_reply":"2024-04-22T23:15:38.499321Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Trainable params: 18350080 || All params: 1539742720 || trainable %: 1.1917627381280946\n","output_type":"stream"}]},{"cell_type":"code","source":"gen_config = model.generation_config\ngen_config.max_new_tokens = 200\ngen_config.temperature = 0.7\ngen_config.top_p = 0.7\ngen_config.num_return_sequences = 1\ngen_config.pad_token_id = tokenizer.eos_token_id\ngen_config.eos_token_id = tokenizer.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:38.501824Z","iopub.execute_input":"2024-04-22T23:15:38.502180Z","iopub.status.idle":"2024-04-22T23:15:38.507173Z","shell.execute_reply.started":"2024-04-22T23:15:38.502147Z","shell.execute_reply":"2024-04-22T23:15:38.506255Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfilename = '/kaggle/input/ind-proj/paths.csv'\n\ndf = pd.read_csv(filename, encoding=\"utf-8\", encoding_errors=\"replace\")\n\ndf = df[['summary', 'paths']]\n\ntrain, test  = train_test_split(df, \n                                train_size=.8,\n                                test_size=.2, \n                                random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:38.508501Z","iopub.execute_input":"2024-04-22T23:15:38.508833Z","iopub.status.idle":"2024-04-22T23:15:40.354337Z","shell.execute_reply.started":"2024-04-22T23:15:38.508800Z","shell.execute_reply":"2024-04-22T23:15:40.353211Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\ntrain_ds = Dataset.from_pandas(train).remove_columns([\"__index_level_0__\"])\n\neval_ds = Dataset.from_pandas(test).remove_columns([\"__index_level_0__\"])\n\npaths_ds = DatasetDict(\n{\n    'train': train_ds,\n    'test': eval_ds\n})","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:40.356217Z","iopub.execute_input":"2024-04-22T23:15:40.356617Z","iopub.status.idle":"2024-04-22T23:15:40.408933Z","shell.execute_reply.started":"2024-04-22T23:15:40.356584Z","shell.execute_reply":"2024-04-22T23:15:40.407967Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \n    \"\"\"\"\n    Update the prompt template:\n    Combine both the prompt and input into a single column.\n\n    \"\"\"     \n    bos_token = \"<s>\"\n    \n    original_system_message = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n    \n    system_message = \"Use the provided context followed by a question to answer it.\"\n    \n    full_prompt = f\"\"\"Act as the author of a Choose Your Own Adventure Book. This book is special as it is based on existing material.\n    Now, as with any choose your own adventure book, you'll have to generate decision paths at certain points in the story.\n    Your job is to generate 4 decision paths for the given point in the story, if applicable to that point in the story.\n    If the given part of the story doesn't contain any decisions from which to generate decision paths, don't\n    generate any. If the given part of the story contains a decision, generate 4 decision paths for that decision.\n    One among the 4 decision paths should be the original path, the other 3 should deviate from the original path in a sensible manner.\n    The decision paths should be generated in a way that they are coherent with the existing story.\n    The result should be a JSON object with the following keys: [text, paths]\n\n    text: The given text\n    paths: The generated decision paths as strings in a list\n    \n    Text:\n    {data_point['summary']}\n    \n    Paths:\n    {eval(data_point['paths'])}\n    \"\"\"\n    \n    full_prompt = \" \".join(full_prompt.split())\n    \n    return full_prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:40.410049Z","iopub.execute_input":"2024-04-22T23:15:40.410351Z","iopub.status.idle":"2024-04-22T23:15:40.416797Z","shell.execute_reply.started":"2024-04-22T23:15:40.410326Z","shell.execute_reply":"2024-04-22T23:15:40.415766Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def generate_and_tokenize_prompt(data_point):\n    \n    full_prompt = generate_prompt(data_point)\n    \n    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n    \n    return tokenized_full_prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:40.417892Z","iopub.execute_input":"2024-04-22T23:15:40.418199Z","iopub.status.idle":"2024-04-22T23:15:40.428986Z","shell.execute_reply.started":"2024-04-22T23:15:40.418176Z","shell.execute_reply":"2024-04-22T23:15:40.428208Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = paths_ds[\"train\"].shuffle().map(generate_and_tokenize_prompt)\nval_dataset = paths_ds[\"test\"].shuffle().map(generate_and_tokenize_prompt)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:40.430020Z","iopub.execute_input":"2024-04-22T23:15:40.430307Z","iopub.status.idle":"2024-04-22T23:15:41.674937Z","shell.execute_reply.started":"2024-04-22T23:15:40.430256Z","shell.execute_reply":"2024-04-22T23:15:41.673957Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/659 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"316db613da764561aaaf7efec5ed1c74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/165 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb0271235044e2fa23305d48c1ee518"}},"metadata":{}}]},{"cell_type":"code","source":"OUTPUT_DIR = \"mistral-gen\"","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:41.676388Z","iopub.execute_input":"2024-04-22T23:15:41.676716Z","iopub.status.idle":"2024-04-22T23:15:41.680994Z","shell.execute_reply.started":"2024-04-22T23:15:41.676687Z","shell.execute_reply":"2024-04-22T23:15:41.680056Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:41.682083Z","iopub.execute_input":"2024-04-22T23:15:41.682383Z","iopub.status.idle":"2024-04-22T23:15:41.693710Z","shell.execute_reply.started":"2024-04-22T23:15:41.682358Z","shell.execute_reply":"2024-04-22T23:15:41.692859Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['summary', 'paths', 'input_ids', 'attention_mask'],\n    num_rows: 659\n})"},"metadata":{}}]},{"cell_type":"code","source":"training_args = transformers.TrainingArguments(\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 4,\n#     num_train_epochs = 1,\n    learning_rate = 2e-4,\n    fp16 = True,\n    save_total_limit = 3,\n    logging_steps = 10,\n    output_dir = OUTPUT_DIR,\n    max_steps = 250,\n    optim = \"paged_adamw_8bit\",\n    lr_scheduler_type = \"cosine\",\n    warmup_ratio = 0.05,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:41.694874Z","iopub.execute_input":"2024-04-22T23:15:41.695199Z","iopub.status.idle":"2024-04-22T23:15:58.621276Z","shell.execute_reply.started":"2024-04-22T23:15:41.695169Z","shell.execute_reply":"2024-04-22T23:15:58.620317Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"2024-04-22 23:15:45.225062: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-22 23:15:45.225178: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-22 23:15:45.479805: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = transformers.Trainer(\n    model = model,\n    train_dataset = train_dataset,\n    eval_dataset = val_dataset,\n    args = training_args,\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:58.622552Z","iopub.execute_input":"2024-04-22T23:15:58.623230Z","iopub.status.idle":"2024-04-22T23:15:58.671201Z","shell.execute_reply.started":"2024-04-22T23:15:58.623201Z","shell.execute_reply":"2024-04-22T23:15:58.670336Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config_use_cache = False\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:15:58.672373Z","iopub.execute_input":"2024-04-22T23:15:58.672652Z","iopub.status.idle":"2024-04-22T23:40:26.835856Z","shell.execute_reply.started":"2024-04-22T23:15:58.672626Z","shell.execute_reply":"2024-04-22T23:40:26.835098Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mn-suneel89\u001b[0m (\u001b[33mn-suneel-duke\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240422_231559-97k6cerl</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/n-suneel-duke/huggingface/runs/97k6cerl' target=\"_blank\">fluent-water-46</a></strong> to <a href='https://wandb.ai/n-suneel-duke/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/n-suneel-duke/huggingface' target=\"_blank\">https://wandb.ai/n-suneel-duke/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/n-suneel-duke/huggingface/runs/97k6cerl' target=\"_blank\">https://wandb.ai/n-suneel-duke/huggingface/runs/97k6cerl</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 24:03, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.593600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.750700</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.211700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.324400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.215500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.137700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.149100</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.217200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.088200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.140500</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.146400</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.109300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.007700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.107300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.074500</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.100900</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.105900</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.068900</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.075500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.061300</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>0.997500</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.011600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.010800</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.985200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.946900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=1.185545139312744, metrics={'train_runtime': 1467.7821, 'train_samples_per_second': 0.681, 'train_steps_per_second': 0.17, 'total_flos': 5260550383656960.0, 'train_loss': 1.185545139312744, 'epoch': 1.5174506828528074})"},"metadata":{}}]},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:26.836935Z","iopub.execute_input":"2024-04-22T23:40:26.837208Z","iopub.status.idle":"2024-04-22T23:40:30.537981Z","shell.execute_reply.started":"2024-04-22T23:40:26.837183Z","shell.execute_reply":"2024-04-22T23:40:30.536954Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1713827759.08d1a6d43a49.34.0:   0%|          | 0.00/11.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73f9b8879d014b1f90c0e91bb1e45213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/73.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06cf5646a72047c6b8094dcc85aad4cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86bb57d5025b4c0b80c27d6dab3ab4fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75bcffbec6824310afe78913b9f927ac"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/suneeln-duke/mistral-gen/commit/20eb322866997494a9e4284eb59fd890e3ad9cda', commit_message='End of training', commit_description='', oid='20eb322866997494a9e4284eb59fd890e3ad9cda', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"def generate_test_prompt(summary):\n    \n    \"\"\"\"\n    Update the prompt template:\n    Combine both the prompt and input into a single column.\n\n    \"\"\"     \n    \n    full_prompt = f\"\"\"Act as the author of a Choose Your Own Adventure Book. This book is special as it is based on existing material.\n    Now, as with any choose your own adventure book, you'll have to generate decision paths at certain points in the story.\n    Your job is to generate 4 decision paths for the given point in the story, if applicable to that point in the story.\n    If the given part of the story doesn't contain any decisions from which to generate decision paths, don't\n    generate any. If the given part of the story contains a decision, generate 4 decision paths for that decision.\n    One among the 4 decision paths should be the original path, the other 3 should deviate from the original path in a sensible manner.\n    The decision paths should be generated in a way that they are coherent with the existing story.\n    The result should be a JSON object with the following keys: [text, paths]\n\n    text: The given text\n    paths: The generated decision paths as strings in a list\n    \n    Text:\n    {summary}\n    \n    Paths:\n    \"\"\"\n    \n    full_prompt = \" \".join(full_prompt.split())\n    \n    return full_prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:30.539503Z","iopub.execute_input":"2024-04-22T23:40:30.539930Z","iopub.status.idle":"2024-04-22T23:40:30.548921Z","shell.execute_reply.started":"2024-04-22T23:40:30.539901Z","shell.execute_reply":"2024-04-22T23:40:30.547633Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\nOUTPUT_DIR = \"mistral-gen\"\n\n#  = pipeline( )\n\n\npath_gen = pipeline(task=\"text-generation\", \n                        model=f\"suneeln-duke/{OUTPUT_DIR}\", \n                        tokenizer=tokenizer,\n                        max_new_tokens = 512, \n                        temperature = 0.0,\n                       )","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:30.550349Z","iopub.execute_input":"2024-04-22T23:40:30.550698Z","iopub.status.idle":"2024-04-22T23:40:37.088843Z","shell.execute_reply.started":"2024-04-22T23:40:30.550666Z","shell.execute_reply":"2024-04-22T23:40:37.087703Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0309c23a27410eae962d41071d019c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01aaa6b19e954d699b6bc852cb0b19b9"}},"metadata":{}}]},{"cell_type":"code","source":"paths_ds['test'][0]","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:37.090126Z","iopub.execute_input":"2024-04-22T23:40:37.090503Z","iopub.status.idle":"2024-04-22T23:40:37.097711Z","shell.execute_reply.started":"2024-04-22T23:40:37.090468Z","shell.execute_reply":"2024-04-22T23:40:37.096826Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"{'summary': 'Lord Bholenath, where will he be in the three worlds? Bhole! Mr. Banwari, have they come? Yes, l know. You dance the entire night, flirt around.. and sleep peacefully during the day . You have got a habit of forgetting. What will we do till they come, sir? You wear a skirt. l will wear pants. We will dance in front of them . Okay, Sir, juice? You drink it. Drink it. Hi, uncle',\n 'paths': \"['Continue dancing and flirting at the party', 'Leave the party early and go home', 'Start a deep conversation with Mr. DP about his house', 'Sneak out of the party to meet GK who is out of lndia']\"}"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = generate_test_prompt(paths_ds['test'][0]['summary'])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:37.098685Z","iopub.execute_input":"2024-04-22T23:40:37.098959Z","iopub.status.idle":"2024-04-22T23:40:37.109971Z","shell.execute_reply.started":"2024-04-22T23:40:37.098935Z","shell.execute_reply":"2024-04-22T23:40:37.109306Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"sample","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:37.111055Z","iopub.execute_input":"2024-04-22T23:40:37.111344Z","iopub.status.idle":"2024-04-22T23:40:37.125412Z","shell.execute_reply.started":"2024-04-22T23:40:37.111320Z","shell.execute_reply":"2024-04-22T23:40:37.124606Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"\"Act as the author of a Choose Your Own Adventure Book. This book is special as it is based on existing material. Now, as with any choose your own adventure book, you'll have to generate decision paths at certain points in the story. Your job is to generate 4 decision paths for the given point in the story, if applicable to that point in the story. If the given part of the story doesn't contain any decisions from which to generate decision paths, don't generate any. If the given part of the story contains a decision, generate 4 decision paths for that decision. One among the 4 decision paths should be the original path, the other 3 should deviate from the original path in a sensible manner. The decision paths should be generated in a way that they are coherent with the existing story. The result should be a JSON object with the following keys: [text, paths] text: The given text paths: The generated decision paths as strings in a list Text: Lord Bholenath, where will he be in the three worlds? Bhole! Mr. Banwari, have they come? Yes, l know. You dance the entire night, flirt around.. and sleep peacefully during the day . You have got a habit of forgetting. What will we do till they come, sir? You wear a skirt. l will wear pants. We will dance in front of them . Okay, Sir, juice? You drink it. Drink it. Hi, uncle Paths:\""},"metadata":{}}]},{"cell_type":"code","source":"%%time\n\nresult = path_gen(sample, pad_token_id=path_gen.tokenizer.eos_token_id)\nanswer = result[0]['generated_text'].split(\"=\")[-1].lower()","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:40:37.130641Z","iopub.execute_input":"2024-04-22T23:40:37.131032Z","iopub.status.idle":"2024-04-22T23:41:01.253444Z","shell.execute_reply.started":"2024-04-22T23:40:37.131000Z","shell.execute_reply":"2024-04-22T23:41:01.252481Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:500: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 47.4 s, sys: 802 ms, total: 48.2 s\nWall time: 24.1 s\n","output_type":"stream"}]},{"cell_type":"code","source":"answer","metadata":{"execution":{"iopub.status.busy":"2024-04-22T23:41:01.254743Z","iopub.execute_input":"2024-04-22T23:41:01.255109Z","iopub.status.idle":"2024-04-22T23:41:01.262962Z","shell.execute_reply.started":"2024-04-22T23:41:01.255080Z","shell.execute_reply":"2024-04-22T23:41:01.261965Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"\"act as the author of a choose your own adventure book. this book is special as it is based on existing material. now, as with any choose your own adventure book, you'll have to generate decision paths at certain points in the story. your job is to generate 4 decision paths for the given point in the story, if applicable to that point in the story. if the given part of the story doesn't contain any decisions from which to generate decision paths, don't generate any. if the given part of the story contains a decision, generate 4 decision paths for that decision. one among the 4 decision paths should be the original path, the other 3 should deviate from the original path in a sensible manner. the decision paths should be generated in a way that they are coherent with the existing story. the result should be a json object with the following keys: [text, paths] text: the given text paths: the generated decision paths as strings in a list text: lord bholenath, where will he be in the three worlds? bhole! mr. banwari, have they come? yes, l know. you dance the entire night, flirt around.. and sleep peacefully during the day . you have got a habit of forgetting. what will we do till they come, sir? you wear a skirt. l will wear pants. we will dance in front of them . okay, sir, juice? you drink it. drink it. hi, uncle paths: ['you wear a skirt.', 'you wear pants.', 'you dance in front of them.', 'you drink it.']\\n\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}